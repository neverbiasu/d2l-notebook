{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch2 预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为完成数据操作， 我们需要某种方法来存储和操作数据。 通常， 我们需要做两件重要的是： 一是获取数据， 二是将数据读入计算机后对其进行操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先来介绍n维数组（n阶数组、具有n个轴的）， 也称为张量（tensor）。 whatever 用哪个深度学习框架， 它都有张量类， 都与NumPy中的ndarray相似。 但深度学习框架支持GPU运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 了解一些会用到的基本的数值计算工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 具有一个轴的张量叫做向量， 两个轴的叫矩阵， 两个以上没有特定名称\n",
    "#### 可以用arange创建一个行向量x\n",
    "#### 张量内的每个值叫做元素（element）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用shape访问张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果质量知道张量中的元素的总数， 即形状中所有元素乘积， 可以检查它的大小（size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要想改变一个张量的形状而不改变元素数量和元素值， 可以调用reshape函数。 形状改变， 但其元素值不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tips: 不需要手写每个维度， torch会自动补全，不想写的那维用-1来代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有时我们想要全0或者全1、 其他常量或者从特征分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2， 3， 4）的张量， 其中所有元素都设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同样，我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值\n",
    "#### randn使每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1574, -1.1357,  0.1220, -0.2893],\n",
       "        [ 0.6371,  1.4886,  0.3351,  0.2703],\n",
       "        [ 0.1449, -0.4756,  0.8136,  0.5345]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。外层的列表对应于轴0，内层的列表对应于轴1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们想在这些数据上执行数学运算，其中最简单且最有用的操作是按元素（elementwise）运算。\n",
    "#### 它们将标准标量运算符应用于数组的每个元素\n",
    "#### 可以基于任何从标量到标量的函数来创建按元素函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在数学表示法中，我们将通过符号$$f : R → R $$来表示一元标量运算符（只接收一个输入）。means一进一出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同样，我们通过符号$$f : R, R → R $$表示二元标量运算符，means该函数接收两个输入，并产生一个输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定同一形状的任意两个向量u和v和二元运算符f，我们可以得到向量$$c = F(u, v)$$。具体计算方法是$$c_i ← f(u_i, v_i)$$，其中ci、ui和vi分别是向量c、u和v中的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按元素运算： 将标准的标量运算符应用于数组的每个元素， 对两个数组按元素做二元运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8]) \n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 像求幂这样的一元运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连接： 可以把多个张量连结(concatenate)在一起,把它们端对端地叠起来形成一个更大的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0) # 沿行连接\n",
    "torch.cat((X, Y), dim=1) # 沿列连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑运算： 可以用来构建二元张量,对每个位置比较两个张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 求和： 对张量的所有元素进行求和,得到一个单元素的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在某些情况下，即使形状不同 ，我们仍然可以通过调用广播机制（broadcasting mechanism）来执行按元素操作，其工作方式如下：\n",
    "##### 1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "##### 2. 对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多数情况下，我们将沿着数组中长度为1的轴进行广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由于维度不匹配，所以会广播成更大的3x2矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 索引和切片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### like python中的数组，张量中的元素可以通过索引访问。且与数组一样，第一个元素的索引是0，最后一个元素索引是‐1；可以指定范围以包含第一个元素和最后一个之前的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用[-1]取最后一个元素， 用[1,3]取第2到3的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 还可以通过指定索引来将元素写入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12 # 冒号表示沿轴的所有元素\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 节省内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行一些操作可能会导致为新结果分配内存\n",
    "##### 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 我们用Python的id()函数演示了这一点，它给我们提供了内存中引用对象的确切地址\n",
    "##### 运行Y = Y + X后，我们会发现id(Y)指向另一个位置，是因为python计算完后为结果分配了新内存再存到Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这不可取，原因有2\n",
    "##### 1:机器学习中可能有数兆的数据，且一秒更新多参，因此我们希望原地执行更新\n",
    "##### 2:如果不原地更新，某些代码可能引用到旧参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以使用切片表示法将操作的结果分配给先前分配的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140227354413552\n",
      "id(Z): 140227354413552\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果在后续计算中没有重复使用X，我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。 ⭐️应该注意+= 和 = + 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 转换为其他Python对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量\n",
    "#### Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. (From Torch Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 深度学习所用来存储和操作的数据是张量（n维数组），提供了运算、广播※、索引和切片、节省内存、转换为python对象等功能。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True, True],\n",
       "         [True, True, True, True],\n",
       "         [True, True, True, True]]),\n",
       " tensor([[False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test1\n",
    "X > Y, X < Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为用深度学习解决现实生活中的问题，我们经常从预处理原始数据开始， 而不是从准备好的张量格式数据开始\n",
    "#### 在Python中常用的数据分析工具中，我们通常用pandas包。与庞大的Python生态系统中许多扩展包一样，pandas可以与张量兼容。因此接下来介绍用pandas预处理原始数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 举个例子， 我们先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv中。处理方式大体适用于其他格式的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join('', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n') # 列名\n",
    "    f.write('NA,Pave,127500\\n') # 每行表示一个数据样本\n",
    "    f.write('6,Liber,106700\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,Rock,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要从创建的CSV文件中加载原始数据集， 我们导入pandas包并调用read_csv函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley   Price\n",
      "0       NaN   Pave  127500\n",
      "1       6.0  Liber  106700\n",
      "2       4.0    NaN  178100\n",
      "3       NaN   Rock  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 处理缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN项代表缺失值。 处理缺失的数据的典型方法包括插值法和删除法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在这里，我们将考虑插值法\n",
    "#### 通过位置索引iloc，我们将data分成inputs和outputs，其中前者为data的前两列，而后者为data的最后一列。对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/15_qt5ln5qn1y1nln4knxp940000gn/T/ipykernel_34046/2829519728.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  inputs = inputs.fillna(inputs.mean())\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Liber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms  Alley\n",
       "0       5.0   Pave\n",
       "1       6.0  Liber\n",
       "2       4.0    NaN\n",
       "3       5.0   Rock"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 因为只有Alley列Liber和NA两类，pandas可以自动将此列转换为两列“Alley_Liber”和“Alley_nan”，因此缺少Alley值的Alley_nan都会设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley_Liber</th>\n",
       "      <th>Alley_Pave</th>\n",
       "      <th>Alley_Rock</th>\n",
       "      <th>Alley_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms  Alley_Liber  Alley_Pave  Alley_Rock  Alley_nan\n",
       "0       5.0            0           1           0          0\n",
       "1       6.0            1           0           0          0\n",
       "2       4.0            0           0           0          1\n",
       "3       5.0            0           0           1          0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 转换为张量格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 0., 1., 0., 0.],\n",
       "         [6., 1., 0., 0., 0.],\n",
       "         [4., 0., 0., 0., 1.],\n",
       "         [5., 0., 0., 1., 0.]], dtype=torch.float64),\n",
       " tensor([127500., 106700., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### pandas包时Python中常用的数据分析工具， 可以与张量兼容\n",
    "#### 处理缺失值时可以用插值或者删除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alley</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pave</td>\n",
       "      <td>127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liber</td>\n",
       "      <td>106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rock</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alley   Price\n",
       "0   Pave  127500\n",
       "1  Liber  106700\n",
       "2    NaN  178100\n",
       "3   Rock  140000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test1\n",
    "cnt = 0\n",
    "max = 0\n",
    "labels = ['NumRooms','Alley','Price']\n",
    "for label in labels:\n",
    "    cnt = data[label].isna().sum() #Detect missing values. 检测缺失值。\n",
    "    if (cnt > max):\n",
    "        max = cnt\n",
    "        delete = label\n",
    "dropped =  data.drop(delete, axis=1)\n",
    "dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       nan, 1.2750e+05],\n",
       "        [6.0000e+00, 1.0670e+05],\n",
       "        [4.0000e+00, 1.7810e+05],\n",
       "        [       nan, 1.4000e+05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_d = data.drop('Alley', axis=1)\n",
    "t_d = torch.tensor(t_d.to_numpy(dtype=float))\n",
    "t_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接下来将简要地回顾一下部分基本线性代数内容\n",
    "#### 介绍线性代数中的基本数学对象、算术和运算，并用数学符号和相应的代码实现来表示它们"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 标量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标量由只有一个元素的张量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量可以被视为标量值组成的列表。这些标量值被称为向量的元素（element）或分量（component）。 tips: 在数学表示中， 向量是粗体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以使用下标来引用向量的任一元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x1, . . . , xn是向量的元素, 在代码中，我们通过张量的索引来访问任一元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 长度、维度和形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 向量只是一个数字数组，也有长度。 \n",
    "##### 在数学表示法中，如果我们想说一个向量x由n个实值标量组成，可以将其表示为 $$x ∈ R^n$$\n",
    "##### 向量的长度通常称为向量的维度（dimension）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 当张量只表示一个向量即只有一个轴时，可用.shape来访问向量长度\n",
    "##### .shape是一个元素组，列出张量每个轴的维度 （横轴竖维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维度（dimension）这个词在不同上下文时往往会有不同的含义\n",
    "##### 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量\n",
    "##### 然而，张量的维度用来表示张量具有的轴数 即 张量.维度 == 张量.轴数,  向量.维度 == 向量.元素数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。\n",
    "#### 矩阵，我们通常用粗体、大写字母来表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数学表示法使用 $$A ∈ R^{m×n}$$ 来表示矩阵A，其由m行和n列的实值标量组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当矩阵具有相同数量的行和列时，其形状将变为正方形；\n",
    "#### 因此，它被称为方阵（square matrix）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以通过行索引（i）和列索引（j）来访问矩阵中的标量元素aij，例如[A]ij。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当我们交换矩阵的行和列时，结果称为矩阵的转置（transpose）\n",
    "#### 通常用 a^⊤ 来表示矩阵的转置，如果 B = A^⊤，则对于任意i和j，都有bij = aji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据\n",
    "#### 尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的行向量更为常见\n",
    "#### 这种约定将支持常见的深度学习实践。如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法 examples：向量是一阶张量， 矩阵是二阶张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数学表示用大写的字母，索引机制与矩阵类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现，其中3个轴对应于高度、宽度，以及一个颜色通道（channel）轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4) #分别是1，2，3维。第一个是batch维，第二个是channel维，第三个是长度维度\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 张量算法的基本性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量有一些实用的属性\n",
    "#### 任何按元素的一元运算都不会改变其操作数的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype = torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 具体而言，两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）。(不同于矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2 \n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以对任意张量进行的一个有用的操作是计算其元素的和\n",
    "#### 元素的总和，可以记为$$\\sum_{i=1}^dxi$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype = torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以表示任意形状张量的元素和。例如，矩阵A中元素的和可以记为$$\\sum_{i=1}^m\\sum_{j=1}^na_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量\n",
    "#### 我们还可以指定张量沿哪一个轴来通过求和降低维度。以矩阵为例，为了通过求和所有行的元素来降维（轴0） //二向箔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指定axis=1将通过汇总所有列的元素降维（轴1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个与求和相关的量是平均值（mean或average）。我们通过将总和除以元素总数来计算平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum()/ A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同样，计算平均值的函数也可以沿指定轴降低张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -非降维求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有时在调用函数来计算总和或均值时保持轴数不变会很有用, 维度没变==轴数不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdim=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 若想沿某个轴计算A元素的累积总和，比如axis=0（按行计算），可以调用cumsum函数。此函数不会沿任何轴降低输入张量的维度，这里是竖着加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 点积（Dot Product）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定两个向量x, y ∈ R^d，它们的点积（dot product）x^⊤y （或⟨x, y⟩）是相同位置的按元素乘积的和：$$x^⊤y =\\sum_{i=1}^d xiyi$$ (应该是x^⊤和y的点积)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定一组由向量x ∈ R^d表示的值，和一组由w ∈ R^d表示的权重。x中的值根据权重w的加权和，可以表示为点积x^⊤w\n",
    "#### 当权重非负且和为1时，点积表示加权平均\n",
    "#### 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 矩阵-向量积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们知道如何计算点积，可以开始理解矩阵-向量积（matrix‐vector product）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以把一个矩阵$A ∈ R^{m×n}$乘法看作一个从$R^n$到$R^m$向量的转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 具体用例有旋转矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  在代码中使用张量表示矩阵‐向量积，我们使用mv函数。 矩阵A和向量x调用torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9 矩阵-矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 掌握点积和矩阵向量积之后， 那么矩阵‐矩阵乘法（matrix‐matrix multiplication）应该很简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要得到矩阵积C = AB， 需要A的行向量*B的列向量（按位运算），即求$c_{ij}$ 的值时，是在求$a_i * b_j$的值（左行右列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵乘法，不应与“Hadamard积”混淆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.10 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数中最有用的一些运算符是范数（norm）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非正式地说，向量的范数是表示一个向量有多大。这里考虑的是分量大小，不是维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在线性代数中，向量范数是将向量映射到标量的函数f。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一个性质是：如果我们按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的**绝对值**缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(αx) = |α|f(x). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二个性质是熟悉的三角不等式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x + y) ≤ f(x) + f(y).  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三个性质简单地说范数必须是非负的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) ≥ 0. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最后一个性质要求范数最小为0，当且仅当向量全由0组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ∀i, [x]i = 0 ⇔ f(x) = 0. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 事实上，欧几里得距离是一个L2范数：假设n维向量**x**中的元素是$x_1, . . . , x_n$，其L2范数是向量元素平方和的平方根：\n",
    "#### $$ ∥x∥_2 = \\sqrt(\\sum{i=1}^n x_i^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 其中，在L2范数中常常省略下标2，也就是说∥x∥等同于∥x∥2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和：\n",
    "#### $$ ∥x∥_1 = \\sum_{i=1}^n|x_i| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2范数和L1范数都是更一般的Lp范数的特例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ∥X∥_F = \\sqrt(\\sum_{i=1}^m\\sum_{i=1}^m x_{ij}^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8990)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -范数和目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.11 关于线性代数的更多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数还有很多，其中很多数学对于机器学习非常有用。例如，矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构。\n",
    "#### 机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。\n",
    "#### 线性代数补充资料(https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵和标、向、张量都是线代基本对象\n",
    "#### 可以通过mean、sum给张量降维\n",
    "#### 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同\n",
    "#### 在深度学习中，我们经常使用范数，如L1范数、L2范数和Frobenius范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test1\n",
    "(A.T).T == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m \u001b[38;5;241m==\u001b[39m (A \u001b[38;5;241m+\u001b[39m B)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# test2\n",
    "# A.T + B.T == (A + B).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb 单元格 200\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb#Y424sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# test3\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb#Y424sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m A \u001b[39m+\u001b[39;49m A\u001b[39m.\u001b[39;49mT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# test3\n",
    "A + A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test4\n",
    "len(X) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test5\n",
    "Xt = torch.arange(12).reshape(3, 4)\n",
    "len(Xt) #第一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb 单元格 203\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb#Y430sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# test6\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nev4rb14su/Desktop/unsupervised_learning/d2l-notebook/ch2.ipynb#Y430sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m A \u001b[39m/\u001b[39;49m A\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# test6\n",
    "# A / A.sum(axis=1) #非单维大小不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " tensor([[ 6, 22, 38],\n",
       "         [54, 70, 86]]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test7\n",
    "Xt2 = torch.arange(24).reshape(2, 3, 4)\n",
    "sum1 = Xt2.sum(axis=0)\n",
    "sum2 = Xt2.sum(axis=1)\n",
    "sum3 = Xt2.sum(axis=2)\n",
    "sum1, sum2, sum3 # 依次对1，2，3维度降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0906, -2.3347, -2.4786, -0.8402, -0.6227],\n",
       "          [ 0.4930,  1.8625,  0.8547, -0.0664, -1.8656],\n",
       "          [ 0.8467,  0.9628,  0.8491,  1.1695, -0.7647],\n",
       "          [ 0.1359,  0.5866,  0.8537, -0.0496, -0.5891]],\n",
       " \n",
       "         [[ 0.5468, -0.0157, -0.1727,  0.4948,  0.3839],\n",
       "          [ 1.1922, -0.8358,  0.1778,  0.2332, -0.0550],\n",
       "          [-0.2160, -0.8824,  1.3889, -1.7350, -1.0066],\n",
       "          [ 1.6809, -0.1444, -0.0804, -1.5644,  1.2192]],\n",
       " \n",
       "         [[ 0.0942, -2.3468,  0.1843, -0.7735,  0.8529],\n",
       "          [ 1.1112, -1.2122, -1.0904,  0.3602, -0.7412],\n",
       "          [-0.5410,  2.7283, -0.8114,  0.1304,  0.4160],\n",
       "          [ 0.5466, -0.3997,  2.4167, -0.3730,  1.7429]]]),\n",
       " tensor(8.5080),\n",
       " tensor([[[8.5080]]]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test8\n",
    "Xt3 = torch.randn(3, 4, 5)\n",
    "norm = torch.linalg.norm(Xt3)\n",
    "norm2 = torch.linalg.norm(Xt3, keepdim=True)\n",
    "Xt3, norm, norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 微积分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在2500年前，古希腊人把一个多边形分成三角形，并把它们的面积相加，才找到计算多边形面积的方法。为了求出曲线形状（比如圆）的面积，古希腊人在这样的形状上刻内接多边形。内接多边形的等长边越多，就越接近圆。这个过程也被称为逼近法（method of exhaustion）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 事实上，逼近法就是积分（integral calculus）的起源\n",
    "#### 2000多年后，微积分的另一支，微分（differential calculus）被发明出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在微分学最重要的应用是优化问题，即考虑如何把事情做到最好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在dl中，我们训练模型，不断更新它们，使它们在看到更多数据时能变得更好\n",
    "#### 就要最小化损失函数的值，损失函数即是衡量模型有多糟糕的指数\n",
    "#### 且我们希望它们遇到新的数据时仍能表现良好，但训练只能在已知数据上拟合\n",
    "### 因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
    "##### • 优化（optimization）：用模型拟合观测数据的过程；\n",
    "##### • 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 导数和微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。在深度学习中，我们通常选择对于模型参数可微的损失函数。\n",
    "#### 简而言之，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 假设我们有一个函数f : R → R，其输入和输出都是标量。如果f的导数存在，这个极限被定义为:\n",
    "#### $$ f^′(x) = \\lim_{h→0} \\frac{f(x + h) − f(x)} {h}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果$f^′(a)$存在，则称$f$在$a$处是可微（differentiable）的\n",
    "#### 如果f在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的\n",
    "#### 导数$f^′(x)$解释为$f(x)$相对于$x$的瞬时（instantaneous）变化率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义$u = f(x) = 3x^2 − 4x$如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from fc import torch as fc\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过令x = 1并让h接近0, 当x = 1时，导数u′是2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit =  2.30000\n",
      "h=0.01000, numerical limit =  2.03000\n",
      "h=0.00100, numerical limit =  2.00300\n",
      "h=0.00010, numerical limit =  2.00030\n",
      "h=0.00001, numerical limit =  2.00003\n"
     ]
    }
   ],
   "source": [
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x))/ h\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:.5f}, numerical limit = {numerical_lim(f, 1, h): .5f}')\n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 熟悉一下导数的几个等价符号。给定y = f(x)，其中x和y分别是函数f的自变量和因变量。以下表达式是等价的：\n",
    "#### $$ f^′(x) = y^′ = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = Dxf(x), $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其中符号 $\\frac{d}{dx} 和$D$是微分运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • $DC = 0$（C是一个常数）\n",
    "#### • $Dx^n = nx^{n−1}$（幂律（power rule），n是任意实数）\n",
    "#### • $De^x = e$\n",
    "#### • $D ln(x) = \\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 假设函数f和g都是可微的，C是一个常数，则：（省略4条公式） 常数相乘法则，加法法则，乘法法则，除法法则\n",
    "$$\\begin{split}\\begin{aligned} \\frac{d}{dx} [C f(x)] & = C \\frac{d}{dx} f(x) && \\textrm{Constant multiple rule} \\\\ \\frac{d}{dx} [f(x) + g(x)] & = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) && \\textrm{Sum rule} \\\\ \\frac{d}{dx} [f(x) g(x)] & = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) && \\textrm{Product rule} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} & = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} && \\textrm{Quotient rule} \\end{aligned}\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在我们可以应用上述几个法则来计算$u'=f'（x）= 3 \\frac{d}{dx}x^2 - 4 \\frac{d}{dx}x = 6x + 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为了对导数的这种解释进行可视化，我们将使用matplotlib\n",
    "#### use_svg_display函数指定matplotlib软件包输出svg图表以获得更清晰的图像\n",
    "#### __注释#@save是一个特殊的标记，会将对应的函数、类或语句保存在fc包__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display(): #@save\n",
    "    '''使用svg格式显示绘图'''\n",
    "    backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们定义set_figsize函数来设置图表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_figsize(figsize=(3.5, 2.5)): #@save\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    fc.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set_axes函数用于设置由matplotlib生成图表的轴的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplot的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以定义一个plot来叠加多条曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"绘制数据点.\"\"\"\n",
    "\n",
    "    def has_one_axis(X):  # 如果X有一个轴\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X): X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    if axes is None:\n",
    "        axes = fc.plt.gca()\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在我们可以绘制$u=f(x)$及其切线了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"239.254688pt\" height=\"177.55pt\" viewBox=\"0 0 239.254688 177.55\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-12-15T11:37:55.568637</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 177.55 \n",
       "L 239.254688 177.55 \n",
       "L 239.254688 0 \n",
       "L 0 0 \n",
       "L 0 177.55 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 36.754688 143.1 \n",
       "L 232.054688 143.1 \n",
       "L 232.054688 7.2 \n",
       "L 36.754688 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m32c75a64a5\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m32c75a64a5\" x=\"45.63196\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(43.13196 156.935938) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"SimHei-30\" d=\"M 2975 2250 \n",
       "Q 2975 1350 2650 700 \n",
       "Q 2325 50 1600 50 \n",
       "Q 875 50 537 700 \n",
       "Q 200 1350 200 2250 \n",
       "Q 200 3150 537 3787 \n",
       "Q 875 4425 1600 4425 \n",
       "Q 2325 4425 2650 3787 \n",
       "Q 2975 3150 2975 2250 \n",
       "z\n",
       "M 2375 2250 \n",
       "Q 2375 3050 2187 3500 \n",
       "Q 2000 3950 1600 3950 \n",
       "Q 1200 3950 1000 3500 \n",
       "Q 800 3050 800 2250 \n",
       "Q 800 1450 1000 987 \n",
       "Q 1200 525 1600 525 \n",
       "Q 2000 525 2187 987 \n",
       "Q 2375 1450 2375 2250 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#SimHei-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m32c75a64a5\" x=\"106.854531\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(104.354531 156.935938) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"SimHei-31\" d=\"M 1950 100 \n",
       "L 1375 100 \n",
       "L 1375 3425 \n",
       "L 625 3425 \n",
       "L 625 3725 \n",
       "Q 1075 3725 1325 3900 \n",
       "Q 1575 4075 1650 4425 \n",
       "L 1950 4425 \n",
       "L 1950 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#SimHei-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m32c75a64a5\" x=\"168.077101\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(165.577101 156.935938) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"SimHei-32\" d=\"M 2850 100 \n",
       "L 300 100 \n",
       "L 300 500 \n",
       "Q 450 900 712 1237 \n",
       "Q 975 1575 1475 2000 \n",
       "Q 1850 2325 2012 2600 \n",
       "Q 2175 2875 2175 3200 \n",
       "Q 2175 3525 2037 3737 \n",
       "Q 1900 3950 1600 3950 \n",
       "Q 1350 3950 1162 3725 \n",
       "Q 975 3500 975 2925 \n",
       "L 400 2925 \n",
       "Q 425 3650 737 4037 \n",
       "Q 1050 4425 1625 4425 \n",
       "Q 2175 4425 2475 4087 \n",
       "Q 2775 3750 2775 3175 \n",
       "Q 2775 2700 2500 2350 \n",
       "Q 2225 2000 1825 1650 \n",
       "Q 1375 1250 1200 1050 \n",
       "Q 1025 850 875 575 \n",
       "L 2850 575 \n",
       "L 2850 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#SimHei-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m32c75a64a5\" x=\"229.299672\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(226.799672 156.935938) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"SimHei-33\" d=\"M 2825 1300 \n",
       "Q 2825 725 2462 387 \n",
       "Q 2100 50 1550 50 \n",
       "Q 1000 50 637 387 \n",
       "Q 275 725 275 1425 \n",
       "L 850 1425 \n",
       "Q 850 950 1037 737 \n",
       "Q 1225 525 1550 525 \n",
       "Q 1875 525 2050 725 \n",
       "Q 2225 925 2225 1350 \n",
       "Q 2225 1700 2037 1900 \n",
       "Q 1850 2100 1375 2100 \n",
       "L 1375 2525 \n",
       "Q 1775 2525 1962 2725 \n",
       "Q 2150 2925 2150 3325 \n",
       "Q 2150 3625 2012 3800 \n",
       "Q 1875 3975 1575 3975 \n",
       "Q 1275 3975 1112 3762 \n",
       "Q 950 3550 925 3150 \n",
       "L 375 3150 \n",
       "Q 425 3725 737 4075 \n",
       "Q 1050 4425 1575 4425 \n",
       "Q 2125 4425 2425 4112 \n",
       "Q 2725 3800 2725 3350 \n",
       "Q 2725 2925 2575 2687 \n",
       "Q 2425 2450 2075 2325 \n",
       "Q 2425 2250 2625 1975 \n",
       "Q 2825 1700 2825 1300 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#SimHei-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(131.904688 169.060938) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"SimHei-78\" d=\"M 2900 100 \n",
       "L 2300 100 \n",
       "L 1575 1175 \n",
       "L 850 100 \n",
       "L 250 100 \n",
       "L 1275 1525 \n",
       "L 325 2900 \n",
       "L 925 2900 \n",
       "L 1575 1850 \n",
       "L 2225 2900 \n",
       "L 2825 2900 \n",
       "L 1875 1525 \n",
       "L 2900 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#SimHei-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path id=\"m850ad9d577\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m850ad9d577\" x=\"36.754688\" y=\"114.635514\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(24.754688 118.053482) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#SimHei-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m850ad9d577\" x=\"36.754688\" y=\"77.490157\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(24.754688 80.908126) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"SimHei-35\" d=\"M 2825 1650 \n",
       "Q 2825 900 2462 475 \n",
       "Q 2100 50 1500 50 \n",
       "Q 975 50 637 400 \n",
       "Q 300 750 275 1350 \n",
       "L 850 1350 \n",
       "Q 850 975 1025 750 \n",
       "Q 1200 525 1525 525 \n",
       "Q 1850 525 2037 800 \n",
       "Q 2225 1075 2225 1650 \n",
       "Q 2225 2150 2062 2387 \n",
       "Q 1900 2625 1625 2625 \n",
       "Q 1400 2625 1237 2525 \n",
       "Q 1075 2425 925 2175 \n",
       "L 425 2175 \n",
       "L 575 4375 \n",
       "L 2725 4375 \n",
       "L 2725 3900 \n",
       "L 1050 3900 \n",
       "L 950 2750 \n",
       "Q 1100 2900 1275 2975 \n",
       "Q 1450 3050 1750 3050 \n",
       "Q 2225 3050 2525 2687 \n",
       "Q 2825 2325 2825 1650 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#SimHei-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m850ad9d577\" x=\"36.754688\" y=\"40.344801\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(19.754688 43.76277) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#SimHei-31\"/>\n",
       "       <use xlink:href=\"#SimHei-30\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.465625 85.15) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"SimHei-66\" d=\"M 2875 3800 \n",
       "Q 2750 3875 2575 3925 \n",
       "Q 2400 3975 2125 3975 \n",
       "Q 1850 3975 1750 3837 \n",
       "Q 1650 3700 1650 3500 \n",
       "L 1650 2900 \n",
       "L 2675 2900 \n",
       "L 2675 2500 \n",
       "L 1650 2500 \n",
       "L 1650 100 \n",
       "L 1150 100 \n",
       "L 1150 2500 \n",
       "L 300 2500 \n",
       "L 300 2900 \n",
       "L 1150 2900 \n",
       "L 1150 3475 \n",
       "Q 1150 3925 1425 4175 \n",
       "Q 1700 4425 2150 4425 \n",
       "Q 2425 4425 2587 4387 \n",
       "Q 2750 4350 2875 4300 \n",
       "L 2875 3800 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-28\" d=\"M 3000 -225 \n",
       "L 2725 -500 \n",
       "Q 2100 100 1787 750 \n",
       "Q 1475 1400 1475 2225 \n",
       "Q 1475 3050 1787 3700 \n",
       "Q 2100 4350 2725 4975 \n",
       "L 3000 4700 \n",
       "Q 2425 4150 2137 3562 \n",
       "Q 1850 2975 1850 2225 \n",
       "Q 1850 1475 2137 887 \n",
       "Q 2425 300 3000 -225 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-29\" d=\"M 1700 2225 \n",
       "Q 1700 1400 1387 750 \n",
       "Q 1075 100 450 -500 \n",
       "L 175 -225 \n",
       "Q 750 300 1037 887 \n",
       "Q 1325 1475 1325 2225 \n",
       "Q 1325 2975 1037 3562 \n",
       "Q 750 4150 175 4700 \n",
       "L 450 4975 \n",
       "Q 1075 4350 1387 3700 \n",
       "Q 1700 3050 1700 2225 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#SimHei-66\"/>\n",
       "      <use xlink:href=\"#SimHei-28\" x=\"50\"/>\n",
       "      <use xlink:href=\"#SimHei-78\" x=\"100\"/>\n",
       "      <use xlink:href=\"#SimHei-29\" x=\"150\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 45.63196 114.635514 \n",
       "L 51.754217 117.38427 \n",
       "L 57.876474 119.687282 \n",
       "L 63.998731 121.54455 \n",
       "L 70.120988 122.956073 \n",
       "L 76.243245 123.921853 \n",
       "L 82.365503 124.441888 \n",
       "L 88.48776 124.516178 \n",
       "L 94.610017 124.144725 \n",
       "L 100.732274 123.327527 \n",
       "L 106.854531 122.064585 \n",
       "L 112.976788 120.355898 \n",
       "L 119.099045 118.201468 \n",
       "L 125.221302 115.601293 \n",
       "L 131.343559 112.555374 \n",
       "L 137.465816 109.06371 \n",
       "L 143.588073 105.126302 \n",
       "L 149.71033 100.74315 \n",
       "L 155.832587 95.914254 \n",
       "L 161.954844 90.639614 \n",
       "L 168.077101 84.919229 \n",
       "L 174.199358 78.7531 \n",
       "L 180.321615 72.141226 \n",
       "L 186.443872 65.083608 \n",
       "L 192.56613 57.580247 \n",
       "L 198.688387 49.63114 \n",
       "L 204.810644 41.23629 \n",
       "L 210.932901 32.395695 \n",
       "L 217.055158 23.109356 \n",
       "L 223.177415 13.377273 \n",
       "\" clip-path=\"url(#pf840fb5233)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 45.63196 136.922727 \n",
       "L 51.754217 135.436913 \n",
       "L 57.876474 133.951099 \n",
       "L 63.998731 132.465285 \n",
       "L 70.120988 130.97947 \n",
       "L 76.243245 129.493656 \n",
       "L 82.365503 128.007842 \n",
       "L 88.48776 126.522028 \n",
       "L 94.610017 125.036213 \n",
       "L 100.732274 123.550399 \n",
       "L 106.854531 122.064585 \n",
       "L 112.976788 120.578771 \n",
       "L 119.099045 119.092956 \n",
       "L 125.221302 117.607142 \n",
       "L 131.343559 116.121328 \n",
       "L 137.465816 114.635514 \n",
       "L 143.588073 113.149699 \n",
       "L 149.71033 111.663885 \n",
       "L 155.832587 110.178071 \n",
       "L 161.954844 108.692257 \n",
       "L 168.077101 107.206442 \n",
       "L 174.199358 105.720628 \n",
       "L 180.321615 104.234814 \n",
       "L 186.443872 102.749 \n",
       "L 192.56613 101.263185 \n",
       "L 198.688387 99.777371 \n",
       "L 204.810644 98.291557 \n",
       "L 210.932901 96.805743 \n",
       "L 217.055158 95.319928 \n",
       "L 223.177415 93.834114 \n",
       "\" clip-path=\"url(#pf840fb5233)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 36.754688 143.1 \n",
       "L 36.754688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 232.054688 143.1 \n",
       "L 232.054688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 36.754688 143.1 \n",
       "L 232.054688 143.1 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 36.754688 7.2 \n",
       "L 232.054688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 43.754688 42.817188 \n",
       "L 165.754687 42.817188 \n",
       "Q 167.754687 42.817188 167.754687 40.817188 \n",
       "L 167.754687 14.2 \n",
       "Q 167.754687 12.2 165.754687 12.2 \n",
       "L 43.754688 12.2 \n",
       "Q 41.754688 12.2 41.754688 14.2 \n",
       "L 41.754688 40.817188 \n",
       "Q 41.754688 42.817188 43.754688 42.817188 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_10\">\n",
       "     <path d=\"M 45.754688 19.965625 \n",
       "L 55.754688 19.965625 \n",
       "L 65.754688 19.965625 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(73.754688 23.465625) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#SimHei-66\"/>\n",
       "      <use xlink:href=\"#SimHei-28\" x=\"50\"/>\n",
       "      <use xlink:href=\"#SimHei-78\" x=\"100\"/>\n",
       "      <use xlink:href=\"#SimHei-29\" x=\"150\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_11\">\n",
       "     <path d=\"M 45.754688 34.028125 \n",
       "L 55.754688 34.028125 \n",
       "L 65.754688 34.028125 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Tangent line (x=1) -->\n",
       "     <g transform=\"translate(73.754688 37.528125) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"SimHei-54\" d=\"M 2875 3900 \n",
       "L 1875 3900 \n",
       "L 1875 100 \n",
       "L 1300 100 \n",
       "L 1300 3900 \n",
       "L 300 3900 \n",
       "L 300 4375 \n",
       "L 2875 4375 \n",
       "L 2875 3900 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-61\" d=\"M 2900 100 \n",
       "L 2300 100 \n",
       "Q 2250 150 2225 237 \n",
       "Q 2200 325 2200 450 \n",
       "Q 2025 250 1775 150 \n",
       "Q 1525 50 1250 50 \n",
       "Q 850 50 575 250 \n",
       "Q 300 450 300 825 \n",
       "Q 300 1200 550 1425 \n",
       "Q 800 1650 1325 1725 \n",
       "Q 1675 1775 1937 1850 \n",
       "Q 2200 1925 2200 2050 \n",
       "Q 2200 2200 2087 2350 \n",
       "Q 1975 2500 1600 2500 \n",
       "Q 1300 2500 1162 2387 \n",
       "Q 1025 2275 975 2075 \n",
       "L 425 2075 \n",
       "Q 475 2475 787 2712 \n",
       "Q 1100 2950 1600 2950 \n",
       "Q 2150 2950 2425 2700 \n",
       "Q 2700 2450 2700 2000 \n",
       "L 2700 625 \n",
       "Q 2700 475 2750 350 \n",
       "Q 2800 225 2900 100 \n",
       "z\n",
       "M 2200 1025 \n",
       "L 2200 1525 \n",
       "Q 2050 1475 1912 1437 \n",
       "Q 1775 1400 1450 1350 \n",
       "Q 1075 1300 962 1175 \n",
       "Q 850 1050 850 875 \n",
       "Q 850 725 962 612 \n",
       "Q 1075 500 1300 500 \n",
       "Q 1525 500 1787 625 \n",
       "Q 2050 750 2200 1025 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-6e\" d=\"M 2825 100 \n",
       "L 2325 100 \n",
       "L 2325 1900 \n",
       "Q 2325 2200 2175 2375 \n",
       "Q 2025 2550 1775 2550 \n",
       "Q 1450 2550 1162 2212 \n",
       "Q 875 1875 875 1375 \n",
       "L 875 100 \n",
       "L 375 100 \n",
       "L 375 2900 \n",
       "L 875 2900 \n",
       "L 875 2375 \n",
       "Q 1075 2650 1312 2800 \n",
       "Q 1550 2950 1925 2950 \n",
       "Q 2375 2950 2600 2700 \n",
       "Q 2825 2450 2825 2075 \n",
       "L 2825 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-67\" d=\"M 3000 2475 \n",
       "Q 2825 2525 2662 2537 \n",
       "Q 2500 2550 2325 2500 \n",
       "Q 2400 2425 2450 2312 \n",
       "Q 2500 2200 2500 1975 \n",
       "Q 2500 1550 2212 1275 \n",
       "Q 1925 1000 1500 1000 \n",
       "Q 1400 1000 1237 1037 \n",
       "Q 1075 1075 975 1125 \n",
       "Q 900 1075 875 1025 \n",
       "Q 850 975 850 900 \n",
       "Q 850 775 1025 712 \n",
       "Q 1200 650 1650 650 \n",
       "Q 2375 650 2637 450 \n",
       "Q 2900 250 2900 -50 \n",
       "Q 2900 -450 2512 -637 \n",
       "Q 2125 -825 1600 -825 \n",
       "Q 925 -825 600 -650 \n",
       "Q 275 -475 275 -175 \n",
       "Q 275 -25 400 125 \n",
       "Q 525 275 725 375 \n",
       "Q 575 450 487 562 \n",
       "Q 400 675 400 850 \n",
       "Q 400 1000 512 1112 \n",
       "Q 625 1225 775 1300 \n",
       "Q 650 1425 575 1600 \n",
       "Q 500 1775 500 1975 \n",
       "Q 500 2400 787 2675 \n",
       "Q 1075 2950 1500 2950 \n",
       "Q 1725 2950 1887 2887 \n",
       "Q 2050 2825 2175 2700 \n",
       "Q 2375 2850 2575 2912 \n",
       "Q 2775 2975 3000 2950 \n",
       "L 3000 2475 \n",
       "z\n",
       "M 2000 1975 \n",
       "Q 2000 2225 1875 2375 \n",
       "Q 1750 2525 1500 2525 \n",
       "Q 1250 2525 1125 2375 \n",
       "Q 1000 2225 1000 1975 \n",
       "Q 1000 1725 1125 1575 \n",
       "Q 1250 1425 1500 1425 \n",
       "Q 1750 1425 1875 1575 \n",
       "Q 2000 1725 2000 1975 \n",
       "z\n",
       "M 2425 -100 \n",
       "Q 2425 0 2312 100 \n",
       "Q 2200 200 1750 200 \n",
       "Q 1650 200 1475 212 \n",
       "Q 1300 225 1075 250 \n",
       "Q 875 175 800 75 \n",
       "Q 725 -25 725 -125 \n",
       "Q 725 -275 925 -375 \n",
       "Q 1125 -475 1625 -475 \n",
       "Q 2050 -475 2237 -362 \n",
       "Q 2425 -250 2425 -100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-65\" d=\"M 2875 1050 \n",
       "Q 2825 600 2475 325 \n",
       "Q 2125 50 1650 50 \n",
       "Q 1050 50 662 437 \n",
       "Q 275 825 275 1500 \n",
       "Q 275 2175 662 2562 \n",
       "Q 1050 2950 1650 2950 \n",
       "Q 2175 2950 2512 2612 \n",
       "Q 2850 2275 2850 1500 \n",
       "L 825 1500 \n",
       "Q 825 950 1062 725 \n",
       "Q 1300 500 1650 500 \n",
       "Q 1925 500 2100 637 \n",
       "Q 2275 775 2325 1050 \n",
       "L 2875 1050 \n",
       "z\n",
       "M 2275 1900 \n",
       "Q 2225 2250 2050 2387 \n",
       "Q 1875 2525 1600 2525 \n",
       "Q 1350 2525 1150 2387 \n",
       "Q 950 2250 850 1900 \n",
       "L 2275 1900 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-74\" d=\"M 2775 175 \n",
       "Q 2650 125 2487 87 \n",
       "Q 2325 50 2050 50 \n",
       "Q 1600 50 1325 300 \n",
       "Q 1050 550 1050 1000 \n",
       "L 1050 2500 \n",
       "L 200 2500 \n",
       "L 200 2900 \n",
       "L 1050 2900 \n",
       "L 1050 3875 \n",
       "L 1550 3875 \n",
       "L 1550 2900 \n",
       "L 2575 2900 \n",
       "L 2575 2500 \n",
       "L 1550 2500 \n",
       "L 1550 975 \n",
       "Q 1550 775 1650 637 \n",
       "Q 1750 500 2025 500 \n",
       "Q 2300 500 2475 550 \n",
       "Q 2650 600 2775 675 \n",
       "L 2775 175 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-6c\" d=\"M 1850 100 \n",
       "L 1350 100 \n",
       "L 1350 4375 \n",
       "L 1850 4375 \n",
       "L 1850 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-69\" d=\"M 1825 3700 \n",
       "L 1325 3700 \n",
       "L 1325 4350 \n",
       "L 1825 4350 \n",
       "L 1825 3700 \n",
       "z\n",
       "M 1825 100 \n",
       "L 1325 100 \n",
       "L 1325 2900 \n",
       "L 1825 2900 \n",
       "L 1825 100 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"SimHei-3d\" d=\"M 3000 2875 \n",
       "L 150 2875 \n",
       "L 150 3275 \n",
       "L 3000 3275 \n",
       "L 3000 2875 \n",
       "z\n",
       "M 3000 1350 \n",
       "L 150 1350 \n",
       "L 150 1750 \n",
       "L 3000 1750 \n",
       "L 3000 1350 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#SimHei-54\"/>\n",
       "      <use xlink:href=\"#SimHei-61\" x=\"50\"/>\n",
       "      <use xlink:href=\"#SimHei-6e\" x=\"100\"/>\n",
       "      <use xlink:href=\"#SimHei-67\" x=\"150\"/>\n",
       "      <use xlink:href=\"#SimHei-65\" x=\"200\"/>\n",
       "      <use xlink:href=\"#SimHei-6e\" x=\"250\"/>\n",
       "      <use xlink:href=\"#SimHei-74\" x=\"300\"/>\n",
       "      <use xlink:href=\"#SimHei-20\" x=\"350\"/>\n",
       "      <use xlink:href=\"#SimHei-6c\" x=\"400\"/>\n",
       "      <use xlink:href=\"#SimHei-69\" x=\"450\"/>\n",
       "      <use xlink:href=\"#SimHei-6e\" x=\"500\"/>\n",
       "      <use xlink:href=\"#SimHei-65\" x=\"550\"/>\n",
       "      <use xlink:href=\"#SimHei-20\" x=\"600\"/>\n",
       "      <use xlink:href=\"#SimHei-28\" x=\"650\"/>\n",
       "      <use xlink:href=\"#SimHei-78\" x=\"700\"/>\n",
       "      <use xlink:href=\"#SimHei-3d\" x=\"750\"/>\n",
       "      <use xlink:href=\"#SimHei-31\" x=\"800\"/>\n",
       "      <use xlink:href=\"#SimHei-29\" x=\"850\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pf840fb5233\">\n",
       "   <rect x=\"36.754688\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.偏导数和梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍多元函数的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 令f为带有n变量的函数。 $y$相对于其$i^{th}$参数$x^i$的偏导数为:\n",
    "#### $$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为了计算$\\frac{\\partial y}{\\partial x_i}$，我们可将$x_1...x_n$视为常量，并计算$y$相对于$x_i$的导数\n",
    "#### $$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = \\partial_{x_i} f = \\partial_i f = f_{x_i} = f_i = D_i f = D_{x_i} f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以将多元函数相对于其所有变量的偏导数连接起来以获得称为函数梯度的向量\n",
    "#### 假设函数$y$的输入是$n$维向量$X$ ，输出是标量\n",
    "#### 函数$f$相对于$X$的梯度是$n$偏导数的向量：\n",
    "#### $$ \\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\left[\\partial_{x_1} f(\\mathbf{x}), \\partial_{x_2} f(\\mathbf{x}), \\ldots\\partial_{x_n} f(\\mathbf{x})\\right]^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有歧义， $\\nabla_{\\mathbf{x}}f(\\mathbf{x})$通常会替换为$\\nabla_f(\\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于所有$A ∈ R^{m×n}$，都有$∇xAx = A⊤$\n",
    "#### 对于所有$A ∈ R^{n×m}$，都有$∇_xx ⊤A = A$\n",
    "#### 对于所有$A ∈ R^{n×n}，都有$∇_xx⊤Ax = (A + A^⊤)x$\n",
    "#### $∇_x∥x∥2 = ∇_xx^⊤x = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4.链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在深度学习中，所关注的梯度通常很难计算，因为我们正在使用深度嵌套的函数\n",
    "#### 幸运的是，链式法则可以解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
    "#### 更多的u和x可以无限链下去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以常规应用微分的基本法则计算梯度\n",
    "#### 这项任务不需要动脑，所以认知力可以用在别处\n",
    "#### 计算向量值函数的导数需要我们跟踪从输出到输入的变量依赖图时乘以矩阵\n",
    "#### 特别时，当我们评估函数时，该图会前向遍历，计算梯度时，向后遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plot(x, [f(x), f(x) \u001b[38;5;241m*\u001b[39m (\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf(x)\u001b[39m\u001b[38;5;124m'\u001b[39m, legend\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf(x)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTangent Line(x=1)\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# test3\n",
    "def f(x):\n",
    "    return x ** x\n",
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), f(x) * (math.log(x, 10) + 1)], 'x', 'f(x)', legend=['f(x)', 'Tangent Line(x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代深度学习框架都通过提供自动微分（通常缩写为 autograd）来完成计算微分\n",
    "#### 当我们通过每个连续函数传递数据时，框架会构建一个计算图来跟踪每个值如何依赖于其他值\n",
    "#### 自动微分通过应用链式法则的图表向后进行——反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd库在过去十年中已成为热门话题，但它们却有着悠久的历史\n",
    "#### 在探索方法之前，我们首先要掌握 autograd 包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1.一个简单的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 假设我们感兴趣的是函数$y=2x^Tx$相对于列向量$x$的微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在我们计算y关于x的梯度之前，需要一个地方来存储梯度\n",
    "#### 注意，一个标量函数关于向量 __x__ 的梯度是向量，并且与 __x__ 具有相同的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们可以通过调用 backward() 方法来获取 $y$ 相对于 $x$ 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证自动梯度计算和预期结果是否相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
