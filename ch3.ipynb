{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们将介绍神经网络的整个训练过程，包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型\n",
    "### 我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归是为单个或多个因变量和自变量之间关系建模的一类方法\n",
    "#### 在一些科学领域，回归经常用来表示输入和输出之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在机器学习领域中的大多数任务通常都与预测（prediction）有关\n",
    "#### 但不是所有预测都是回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 线性回归的基本元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归（linear regression）可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归基于几个基本的假设：\n",
    "#### 1. 首先，假设自变量$\\chi$和因变量$y$的关系是线性的，即$y$可以表示为$\\chi$中元素的加权和\n",
    "#### 2. 这里通常允许包含观测值的一些噪声，其次，我们假设任何噪声都比较正常，如噪声遵循正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为开发一个能预测__的模型，我们需要数据集，在ml中称为训练数据集（training dataset）\n",
    "#### 每行数据称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）\n",
    "#### 预测的目标叫做标签（label）或者目标（target）。\n",
    "#### 预测所依据的自变量叫做特征（feature）或者协变量（covariate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通常n表示数据集中的样本数。对索引为i的样本其输入表示为$x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^⊤$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 每个解决方案的核心都是一个模型，该模型描述如何将特征转换为目标的估计\n",
    "#### 线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\textrm{price} = w_{\\textrm{area}} \\cdot \\textrm{area} + w_{\\textrm{age}} \\cdot \\textrm{age} + b.\\textrm{price} = w_{\\textrm{area}} \\cdot \\textrm{area} + w_{\\textrm{age}} \\cdot \\textrm{age} + b. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $w_{area}$ 和 $w_{age}$称为权重（weight），权重决定了每个特征对我们预测值的影响\n",
    "#### $b$称为偏置（bias）、偏移量（offset）或截距（intercept）\n",
    "#### 偏置是指当所有特征都取值为0时，预测值应该为多少\n",
    "#### 如果没有偏置项，我们模型的表达能力会出问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 严格来说，这个公式是输入特征的一个仿射变换（affine transformation）\n",
    "#### 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项进行平移（translation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们的目标是找到权重$w$和偏置$b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在机器学习中，我们的数据集通常都是高维的，建模时采用线性代数表示法更方便\n",
    "#### 当我们输入包含$d$个特征时，我们将预测结果$\\hat y$表示为\n",
    "#### $$ \\hat{y} = w_1  x_1 + \\cdots + w_d  x_d + b. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将所有特征收集到$\\chi \\in \\mathbb R^d$中，可以通过点积来表示公式\n",
    "#### $$ \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量$\\chi$对应于单个数据样本的特征\n",
    "#### 用符号表示的矩阵$X \\in R^{n×d}$ 可以很方便地引用我们整个数据集的n个样本\n",
    "#### 其中，X的每一行是一个样本，每一列是一种特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于特征集合X，预测值$\\hat y \\in R^n $可以通过矩阵‐向量乘法表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ {\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这个过程中的求和将使用广播机制\n",
    "#### 线性回归的目标是找到一组权重向量w和偏置b能够使得新样本预测标签的误差尽可能小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 无论如何，都可能会出现少量的观测误差。因此，即使确信特征与标签的潜在关系是线性的，我们也会加入一个噪声项来考虑观测误差带来的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在开始寻找最好的模型参数（model parameters）w和b之前，我们还需要两个东西：（1）一种模型质量的度量方式；（2）一种能够更新模型以提高模型预测质量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数和平方误差\n",
    "#### 在线性回归中，损失函数用于量化模型预测值与实际标签之间的差距。通常选择非负数作为损失，数值越小表示拟合效果越好，完美预测的损失为0。回归问题中常用的损失函数是平方误差函数，定义为：\n",
    "#### $$ l^{(i)}(w, b) = \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2 $$\n",
    "\n",
    "#### 平方误差函数的特点\n",
    "#### 平方误差函数中的常数 $\\frac{1}{2} $是为了求导时的方便。平方误差函数使得预测值与实际值之间较大的差异导致更大的损失，这有助于模型更好地拟合数据。 \n",
    "<img src = \"https://zh.d2l.ai/_images/fit-linreg.svg\" margin: auto>\n",
    "\n",
    "#### 模型在整个数据集上的质量度量\n",
    "#### 为了度量模型在整个数据集上的质量，计算所有训练样本上的损失均值：\n",
    "#### $$ L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( w^\\top x^{(i)} + b - y^{(i)} \\right)^2 $$\n",
    "\n",
    "#### 模型训练的目标\n",
    "#### 训练模型的目标是找到一组参数 $ w^*, b^* $，使得它们能最小化所有训练样本上的总损失：\n",
    "#### $$ w^*, b^* = \\arg \\min_{w,b} L(w, b) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归不同于其他模型，其解可以用一个公式简单的表示出来，这类解叫作解析解\n",
    "#### 首先，将b合并到**w**中，方式是在包含所有参数的矩阵加一列\n",
    "#### 那我们的预测问题就是最小化$||y-\\chi w||$,在这损失平面只有一个临界点，这个点对应整个区域的损失极小值\n",
    "#### 将损失关于**w**的导数设为0，得到解：\n",
    "#### $$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.4. 随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们用到一种名为梯度下降（gradient descent）的方法，这种方法几乎可以优化所有深度学习模型\n",
    "#### 它通过不断地在损失函数递减的方向上更新参数来降低误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）\n",
    "#### 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集\n",
    "#### so，我们经常会在每次计算更新时随机抽取一小部分的样本，这种变体叫做随机梯度下降（minibatch stochastic gradient descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在每次迭代中，我们首先随机抽样一个小批量$B$。它是由固定数量的训练样本组成的\n",
    "#### 然后我们计算梯度，最后，我们将梯度乘以一个预先确定的正数η，并从当前参数值中减去\n",
    "#### $$ (\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\begin{split}\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $w$和**x**都是向量。在这里，更优雅的向量表示法比系数表示法（如w1, w2, . . . , wd）更具可读性\n",
    "#### $|B|$表示每个小批量中的样本数，这也称为批量大小（batch size）η表示学习率（learning rate）。\n",
    "#### 批量大小和学习率的值通常是手动预先指定\n",
    "#### η表示学习率（learning rate）\n",
    "#### 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。调参（hyperparameter tuning）是选择超参数的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后），我们记录下模型参数的估计值，表示为$\\hat w$ ,$\\hat b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。\n",
    "#### 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用模型进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定“已学习”的线性回归模型$\\hat w^⊤x + \\hat b$，现在我们可以通过房屋面积$x_1$和房龄$x_2$来估计一个（未包含在训练数据中的）新房屋价格\n",
    "#### 给定特征估计目标的过程通常称为预测（prediction）或推断（inference）。 # 推理更好听，其实就是推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 矢量化加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。\n",
    "#### 为实现，需要我们对计算进行矢量化，从而用线性代数库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from fc.torch import fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fc\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fc'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为说明矢量化为何如此重要，我们考虑对向量相加的两种方法\n",
    "#### 实例化两个10000维的向量，第一种用for循环便利，第二种直接+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = 10000\n",
    "a = torch.ones([n])\n",
    "b = torch.ones([n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
